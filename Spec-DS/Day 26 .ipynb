{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8a446f",
   "metadata": {},
   "source": [
    "Speeding the model execution\t\n",
    "    Selecting Best Models When Preprocessing \n",
    "\tSpeeding Up Model Selection with Parallelization\n",
    "\tSpeeding Up Model Selection Using Algorithm-Specific Methods\n",
    "\tEvaluating Performance After Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab939d3",
   "metadata": {},
   "source": [
    "Selecting Best Models When Preprocessing\n",
    "1. Rescale Data  -> MinMaxScaler (Sklearn)\n",
    "2. Binarize Data (Make Binary)  \n",
    "3. Standardize Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c19daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0       6   148    72    35     0  33.6  0.627   50      1\n",
      "1       1    85    66    29     0  26.6  0.351   31      0\n",
      "2       8   183    64     0     0  23.3  0.672   32      1\n",
      "3       1    89    66    23    94  28.1  0.167   21      0\n",
      "4       0   137    40    35   168  43.1  2.288   33      1\n",
      "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
      "763    10   101    76    48   180  32.9  0.171   63      0\n",
      "764     2   122    70    27     0  36.8  0.340   27      0\n",
      "765     5   121    72    23   112  26.2  0.245   30      0\n",
      "766     1   126    60     0     0  30.1  0.349   47      1\n",
      "767     1    93    70    31     0  30.4  0.315   23      0\n",
      "\n",
      "[768 rows x 9 columns]\n",
      "[[0.353 0.744 0.59  0.354 0.    0.501 0.234 0.483]\n",
      " [0.059 0.427 0.541 0.293 0.    0.396 0.117 0.167]\n",
      " [0.471 0.92  0.525 0.    0.    0.347 0.254 0.183]\n",
      " [0.059 0.447 0.541 0.232 0.111 0.419 0.038 0.   ]\n",
      " [0.    0.688 0.328 0.354 0.199 0.642 0.944 0.2  ]]\n"
     ]
    }
   ],
   "source": [
    "#Rescaling (between 0 to 1)\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "url=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names = names)\n",
    "array = dataframe.values\n",
    "print(dataframe)\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "numpy.set_printoptions(precision=3)  \n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44cb8f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0       6   148    72    35     0  33.6  0.627   50      1\n",
      "1       1    85    66    29     0  26.6  0.351   31      0\n",
      "2       8   183    64     0     0  23.3  0.672   32      1\n",
      "3       1    89    66    23    94  28.1  0.167   21      0\n",
      "4       0   137    40    35   168  43.1  2.288   33      1\n",
      "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
      "763    10   101    76    48   180  32.9  0.171   63      0\n",
      "764     2   122    70    27     0  36.8  0.340   27      0\n",
      "765     5   121    72    23   112  26.2  0.245   30      0\n",
      "766     1   126    60     0     0  30.1  0.349   47      1\n",
      "767     1    93    70    31     0  30.4  0.315   23      0\n",
      "\n",
      "[768 rows x 9 columns]\n",
      "[[1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Binarize data ->  Binarizer(Sklearn)\n",
    "\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import Binarizer\n",
    "url=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names = names)\n",
    "array = dataframe.values\n",
    "print(dataframe)\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "binarizer = Binarizer(threshold = 0.0).fit(X)\n",
    "binaryX = binarizer.transform(X)\n",
    "numpy.set_printoptions(precision = 3)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "297f987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0       6   148    72    35     0  33.6  0.627   50      1\n",
      "1       1    85    66    29     0  26.6  0.351   31      0\n",
      "2       8   183    64     0     0  23.3  0.672   32      1\n",
      "3       1    89    66    23    94  28.1  0.167   21      0\n",
      "4       0   137    40    35   168  43.1  2.288   33      1\n",
      "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
      "763    10   101    76    48   180  32.9  0.171   63      0\n",
      "764     2   122    70    27     0  36.8  0.340   27      0\n",
      "765     5   121    72    23   112  26.2  0.245   30      0\n",
      "766     1   126    60     0     0  30.1  0.349   47      1\n",
      "767     1    93    70    31     0  30.4  0.315   23      0\n",
      "\n",
      "[768 rows x 9 columns]\n",
      "[[ 0.64   0.848  0.15   0.907 -0.693  0.204  0.468  1.426]\n",
      " [-0.845 -1.123 -0.161  0.531 -0.693 -0.684 -0.365 -0.191]\n",
      " [ 1.234  1.944 -0.264 -1.288 -0.693 -1.103  0.604 -0.106]\n",
      " [-0.845 -0.998 -0.161  0.155  0.123 -0.494 -0.921 -1.042]\n",
      " [-1.142  0.504 -1.505  0.907  0.766  1.41   5.485 -0.02 ]]\n"
     ]
    }
   ],
   "source": [
    "#3. Standardize Data  ->StandardScaler(sklearn)\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "url=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names = names)\n",
    "array = dataframe.values\n",
    "print(dataframe)\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "numpy.set_printoptions(precision = 3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dcb70d",
   "metadata": {},
   "source": [
    "#Speeding Up Model Selection with Parallelization\n",
    "\n",
    "100 -> split to 10 parts and run parallely -> combine to create a single model\n",
    "\n",
    "\n",
    "1.Using Pandas directly with two threads\n",
    "2.Using Dask with threads and separate processes\n",
    "3.Using Modin with a Ray/ Dask backend\n",
    "4.Using multiprocessing.Pool to launch separate processes \n",
    "5.Using joblib.parallel to launch separate threads and processes \n",
    "\n",
    "pip install pandas==1.1.4\n",
    "pip install dask==2.16.0\n",
    "pip install modin[ray]==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54aa6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4361f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dask -> Paralell computing concept , Oracle ADS Lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab8eab",
   "metadata": {},
   "source": [
    "os.environ['MODIN_ENGINE'] = \"ray\"\n",
    "import ray \n",
    "ray.init(num_gpus=0)\n",
    "import modin.pandas as mpd\n",
    "modin_df = mpd.DataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64258e",
   "metadata": {},
   "source": [
    "%%time \n",
    "\n",
    "modin_df['col2'] = modin_df['col2'] + 100.0 + random()\n",
    "print(modin_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585569b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Refer :https://github.com/VedaVinothiniShastra/DS_PT_1/blob/main/Spec-DS/Speeding_Up_Model_Selection_with_Parallelization.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e94879",
   "metadata": {},
   "source": [
    "Speeding Up Model Selection Using Algorithm-Specific Methods\n",
    "\n",
    "1.Probabilistic Measures -> Choose a model via in-sample error and complexity\n",
    "\n",
    "    a.Akaike Information Criterion (AIC)->method for scoring and selecting a model\n",
    "        AIC = -2/N * LL + 2 * k/N\n",
    "        LL-> is the log-likelihood of the model\n",
    "        N ->is the number of examples in the training dataset\n",
    "        k ->is the number of parameters in the model\n",
    "    \n",
    "    b.Bayesian Information Criterion (BIC)->the model with the lowest BIC is selected\n",
    "        BIC = -2 * LL + log(N) * k\n",
    "        LL-> is the log-likelihood of the model\n",
    "        N ->is the number of examples in the training dataset\n",
    "        k ->is the number of parameters in the model\n",
    "    \n",
    "    c.Minimum Description Length (MDL)->the model with the lowest MDL is selected\n",
    "        MDL = L(h) + L(D | h)\n",
    "        h -> is the model\n",
    "        D -> is the predictions made by the model\n",
    "        L(h)-> is the number of bits required to represent the model\n",
    "        L(D | h)-> is the number of bits required to represent the predictions from the model on the training dataset.\n",
    "        \n",
    "        \n",
    "    d.Structural Risk Minimization (SRM)->balancing the model's complexity against its success at fitting the training data\n",
    "    \n",
    "2.Resampling Methods ->Choose a model via estimated out-of-sample error\n",
    "    a.Random train/test splits.\n",
    "    b.Cross-Validation (k-fold, LOOCV, etc.).\n",
    "    c.Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5a1dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3\n",
      "0.010108623559815571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=0.1)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "num_params = len(model.coef_) + 1\n",
    "print('Number of parameters: %d' % (num_params))\n",
    "yhat = model.predict(X)\n",
    "mse = mean_squared_error(y, yhat)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c4491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aic(n, mse, num_params):\n",
    " aic = n * log(mse) + 2 * num_params\n",
    " return aic\n",
    "def calculate_bic(n, mse, num_params):\n",
    " bic = n * log(mse) + num_params * log(n)\n",
    " return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba825be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC -453.4366401626476\n",
      "BIC -445.62112960468335\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "aic = calculate_aic(len(y), mse, num_params)\n",
    "print('AIC',aic)\n",
    "bic = calculate_bic(len(y), mse, num_params)\n",
    "print('BIC',bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6035f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "#LOOCV\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_blobs(n_samples=100, random_state=1)\n",
    "cv = LeaveOneOut()\n",
    "y_true, y_pred = list(), list()\n",
    "for train_ix, test_ix in cv.split(X):\n",
    "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    model = RandomForestClassifier(random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(yhat[0])\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbc818a",
   "metadata": {},
   "source": [
    "Evaluating Performance After Model Selection\n",
    "refer : https://github.com/VedaVinothiniShastra/DS_PT_1/blob/main/Spec-DS/Day%2024.ipynb\n",
    "        \n",
    "Classification:\n",
    "    1.Precision\n",
    "    2.Recall\n",
    "    3.F1 Score\n",
    "    4.Confusion Matrix \n",
    "    5.ROC Curve Log loss\n",
    "Regression:\n",
    "    1.MSE\n",
    "    2.MAE\n",
    "    3.RMSE\n",
    "    4.RMSLE\n",
    "    5.R Squared\n",
    "    6.Adjuested R Squared\n",
    "Clustering :\n",
    "    1.Adjusted Rand Index\n",
    "    2.Rand Index\n",
    "    3.Silhouette Score aka Silhouette Coefficient\n",
    "    4.Davies-Bouldin Index ratio \n",
    "    5.Mutual Information\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15031fc8",
   "metadata": {},
   "source": [
    "Neural Networks\t\n",
    "    Introduction\n",
    "\tPreprocessing Data for Neural Networks\n",
    "\tDesigning a Neural Network\n",
    "\tTraining a Binary Classifier\n",
    "Neural Networks Multiclass Classifier\t\n",
    "    Training a Multiclass Classifier\n",
    "\tTraining a Regressor\n",
    "\tMaking Predictions\n",
    "Addressing Overfitting Problem\t\n",
    "    Visualize Training History\n",
    "\tReducing Overfitting with Weight Regularization\n",
    "\tReducing Overfitting with Early Stopping\n",
    "\tReducing Overfitting with Dropout\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea4f45",
   "metadata": {},
   "source": [
    "Neural Networks\t->Inspired by biological neural networks\n",
    "    Introduction\n",
    "    Supervised, Unsupervised \n",
    "    Types of NN:\n",
    "        1.multilayer perceptron ->uses a nonlinear activation function.\n",
    "        2.convolutional neural network CNN ->used multilayer perceptrons.\n",
    "        3.recursive neural network RNN ->uses weights to make structured predictions\n",
    "        4.recurrent neural network  RNN ->use recurrent neural network architecture and does not use an activation function\n",
    "        4.sequence-to-sequence modules (2 Recuurent NN & Shallow Nn)->use two recurrent networks and shallow neural networks which produce a vector space from an amount of text\n",
    "        \n",
    "\tPreprocessing Data for Neural Networks \n",
    "    1.Min-Max Scaling\n",
    "    2.Decimal Scaling\n",
    "    3.Eliminating Outliers\n",
    "    4.Z-score normalization or Standardization\n",
    "    5.Mean / Median Absolute Deviation\n",
    "    6.(Modified) Tanh Estimator\n",
    "    7.Max-Scaling\n",
    "    \n",
    "\tDesigning a Neural Network\n",
    "\tTraining a Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Refer : https://github.com/VedaVinothiniShastra/DS_PT_1/blob/main/Spec-DS/Day%2031%20%26%2032.ipynb \n",
    "        \n",
    "        from Preprocessing Data for Neural Networks to Reducing Overfitting with Dropout\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fe3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04da4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867840c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
